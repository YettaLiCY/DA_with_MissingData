{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "环境: numpy=1.19.2; pandas=1.1.3; lightgbm=3.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 假设train.csv中数据有缺失(即假设自2017.1.1至2020.12.31每天都应该有交易数据):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用三种方式填充缺失值: 1. 用上一年或下一年该月的月均销量(或者年均销量)填充该年该月的缺失值; 2. 缺失值全部用0填充; 3. 用上个月和下个月的权重均值来填充这个月的缺失值(假设[20%, 80%])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为了防止时间浪费, 已附上缺失值处理后的csv文件, 只需读取即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import time\n",
    "start_time = time.time()  # 获取当时系统时间\n",
    "warnings.filterwarnings('ignore')  # 控制警告输出\n",
    "\n",
    "path = ''  # Your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_simple = pd.read_csv(\n",
    "    path + 'train_simple.csv',\n",
    "    parse_dates=['datetime'],\n",
    "    index_col=['datetime'])  # train_simple.csv 是提取了 train.csv 文件里的 item_id, province_id, quantity 列\n",
    "Origin = train_simple.groupby(['item_id', 'province_id', 'datetime']).sum()\n",
    "Origin.to_csv('Origin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = pd.read_csv(path + 'Origin.csv',\n",
    "                     parse_dates=['datetime'],\n",
    "                     index_col=['datetime'])  # 正确读取 datetime 日期列\n",
    "test = pd.read_csv(\n",
    "        path + 'perday.csv',  # perday.csv中的item_id, province_id 与 submission.csv相同, 不同的是 perday.csv 中包含了2021.1-2021.3的每一天\n",
    "        parse_dates=['datetime'],\n",
    "        index_col=['datetime'])\n",
    "rawbase = pd.read_csv(\n",
    "        path + 'rawbase.csv',  # rawbase.csv中的item_id, province_id 与 train.csv相同, 不同的是 rawbase.csv 中包含了2017.1-2020.12的每一天\n",
    "        parse_dates=['datetime'],\n",
    "        index_col=['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(\n",
    "        rawbase,  # 将 rawbase 和 origin 数据框合并起来, 发现缺失值\n",
    "        origin,\n",
    "        on=['datetime', 'item_id', 'province_id'],\n",
    "        how='left')\n",
    "train = train.rename(columns={'quantity_y': 'quantity'})\n",
    "train = train[['item_id', 'province_id', 'quantity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(df):\n",
    "    df['day'] = df.index.day\n",
    "    df['month'] = df.index.month\n",
    "    df['year'] = df.index.year\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    return df\n",
    "\n",
    "\n",
    "data = expand(train)\n",
    "\n",
    "used_data = data.loc[(data.year >= 2017) & (data.year<=2020)] # 此处为使用2017年及以后的数据 (同理可使用2018年及以后的数据; 2020年以前的数据)\n",
    "# used_data = data.loc[(data.year >= 2017) & (data.year <= 2019)]  # 此处为使用2017年至2019年的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 pivot_table 透视表\n",
    "year_itemid_table = pd.pivot_table(used_data,\n",
    "                                   index='year',\n",
    "                                   columns=['item_id', 'province_id', 'month'],\n",
    "                                   values='quantity',\n",
    "                                   aggfunc=np.mean)\n",
    "# 通过 index='year' 获取 year 信息\n",
    "# 通过 values='quantity' 筛选出需要的quantity数据\n",
    "# 通过 aggfunc = np.mean 计算均值\n",
    "# 通过 columns='item_id' 设置列层次字段\n",
    "\n",
    "year_itemid_fillmonth_table = pd.pivot_table(\n",
    "        used_data,\n",
    "        index='year',\n",
    "        columns=['item_id', 'province_id'],\n",
    "        values='quantity',\n",
    "        aggfunc=np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法一: 用上一年或下一年该月的月均销量(或者年均销量)填充该年该月的缺失值(耗时约7分钟)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_itemid_table1 = year_itemid_table.fillna(method='bfill')\n",
    "year_itemid_table1 = year_itemid_table1.fillna(\n",
    "    method='ffill')  # 用上一年该月的月均值或下一年该月的月均值的值填充缺失值\n",
    "\n",
    "year_itemid_fillmonth_table1 = year_itemid_fillmonth_table.fillna(\n",
    "    method='bfill')\n",
    "year_itemid_fillmonth_table1 = year_itemid_fillmonth_table1.fillna(\n",
    "    method='ffill')\n",
    "\n",
    "used_data1 = used_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(indexlist, column) = np.where(np.isnan(used_data1))\n",
    "\n",
    "for index in indexlist:\n",
    "    itemid = used_data1.iloc[index].item_id  # 读取对应的 itemid, provinceid, year, month\n",
    "    provinceid = used_data1.iloc[index].province_id\n",
    "    year = used_data1.iloc[index].year\n",
    "    month = used_data1.iloc[index].month\n",
    "    if itemid not in year_itemid_table1:\n",
    "        used_data1['quantity'][index] = 0  # 如果该itemid没有任何数据, 则按 0 填充缺失值\n",
    "    else:\n",
    "        if provinceid not in year_itemid_table1[int(itemid)]:\n",
    "            used_data1['quantity'][index] = 0  # 如果该provinceid没有任何数据, 则按 0 填充缺失值\n",
    "        else:\n",
    "            if month in year_itemid_table1[int(itemid)][int(provinceid)]:  # 判断月份是否出现在 year_itemid_table1 里\n",
    "                # 如果在就按月均填充\n",
    "                used_data1['quantity'][index] = year_itemid_table1[int(itemid)][int(provinceid)][int(month)].loc[int(year)]  # 在 year_itemid_table1里进行搜索\n",
    "            else:\n",
    "                # 如果不在就按年均填充\n",
    "                used_data1['quantity'][index] = year_itemid_fillmonth_table1[int(itemid)][int(provinceid)].loc[int(year)]\n",
    "\n",
    "used_data1.to_csv(path + 'used_data1.csv')\n",
    "# used_data1.to_csv(path + 'used_data1_ex2020.csv')    # 使用2017-2019年的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法二: 用0填充缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_data2 = used_data.fillna(0)\n",
    "used_data2.to_csv(path + 'used_data2.csv')\n",
    "# used_data2.to_csv(path + 'used_data2_ex2020.csv')    # 使用2017-2019年的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法三: 用上个月和下个月的权重均值来填充这个月的缺失值(假设权重为[20%,80%])(耗时约1分钟)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [0.2, 0.8]\n",
    "year_itemid_table3_b = year_itemid_table.fillna(method='bfill')\n",
    "year_itemid_table3_b = year_itemid_table3_b.fillna(0)\n",
    "year_itemid_table3_f = year_itemid_table.fillna(method='ffill')\n",
    "year_itemid_table3_f = year_itemid_table3_f.fillna(0)\n",
    "year_itemid_table3 = weight[0] * year_itemid_table3_f + weight[1] * year_itemid_table3_b\n",
    "\n",
    "year_itemid_fillmonth_table3_b = year_itemid_fillmonth_table.fillna(method='bfill')\n",
    "year_itemid_fillmonth_table3_b = year_itemid_fillmonth_table3_b.fillna(0)\n",
    "year_itemid_fillmonth_table3_f = year_itemid_fillmonth_table.fillna(method='ffill')\n",
    "year_itemid_fillmonth_table3_f = year_itemid_fillmonth_table3_f.fillna(0)\n",
    "year_itemid_fillmonth_table3 = weight[0] * year_itemid_fillmonth_table3_f + weight[1] * year_itemid_fillmonth_table3_b\n",
    "\n",
    "used_data3 = used_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(indexlist, column) = np.where(np.isnan(used_data3))\n",
    "\n",
    "for index in indexlist:\n",
    "    itemid = used_data3.iloc[index].item_id  # 读取对应的 itemid, provinceid, year, month\n",
    "    provinceid = used_data3.iloc[index].province_id\n",
    "    year = used_data3.iloc[index].year\n",
    "    month = used_data3.iloc[index].month\n",
    "    if itemid not in year_itemid_table3:\n",
    "        used_data3['quantity'][index] = 0  # 如果该itemid没有任何数据, 则按 0 填充缺失值\n",
    "    else:\n",
    "        if provinceid not in year_itemid_table3[int(itemid)]:\n",
    "            used_data3['quantity'][index] = 0  # 如果该provinceid没有任何数据, 则按 0 填充缺失值\n",
    "        else:\n",
    "            if month in year_itemid_table3[int(itemid)][int(provinceid)]:  # 判断月份是否出现在 year_itemid_table1 里\n",
    "                # 如果在就按月均填充\n",
    "                used_data3['quantity'][index] = year_itemid_table3[int(itemid)][int(provinceid)][int(month)].loc[int(year)]  # 在 year_itemid_table1里进行搜索\n",
    "            else:\n",
    "                # 如果不在就按年均填充\n",
    "                used_data3['quantity'][index] = year_itemid_fillmonth_table3[int(itemid)][int(provinceid)].loc[int(year)]\n",
    "\n",
    "used_data3.to_csv(path + 'used_data3.csv')\n",
    "# used_data3.to_csv(path + 'used_data3_ex2020.csv')    # 使用2017-2019年的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取生成的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_data = pd.read_csv(path + 'used_data1.csv', parse_dates=['datetime'], index_col=['datetime'])  # 读取方法一获取的数据\n",
    "# used_data = pd.read_csv(path+'used_data2.csv', parse_dates=['datetime'], index_col=['datetime'])\n",
    "# used_data = pd.read_csv(path+'used_data3.csv', parse_dates=['datetime'], index_col=['datetime'])\n",
    "# used_data = pd.read_csv(path + 'used_data1_ex2020.csv', parse_dates=['datetime'], index_col=['datetime'])\n",
    "# used_data = pd.read_csv(path + 'used_data2_ex2020.csv', parse_dates=['datetime'], index_col=['datetime'])\n",
    "# used_data = pd.read_csv(path + 'used_data3_ex2020.csv', parse_dates=['datetime'], index_col=['datetime'])\n",
    "\n",
    "dayofweek_itemid_table = pd.pivot_table(used_data,\n",
    "                                        index='day_of_week',\n",
    "                                        columns='item_id',\n",
    "                                        values='quantity',\n",
    "                                        aggfunc=np.mean)\n",
    "\n",
    "quanti_avg = used_data.quantity.mean()\n",
    "\n",
    "month_table = pd.pivot_table(\n",
    "    used_data, index='month', values='quantity', aggfunc=np.mean) / quanti_avg\n",
    "\n",
    "provinceid_table = pd.pivot_table(\n",
    "    used_data, index='province_id', values='quantity', aggfunc=np.mean) / quanti_avg\n",
    "\n",
    "year_table = pd.pivot_table(\n",
    "    used_data, index='year', values='quantity', aggfunc=np.mean) / quanti_avg\n",
    "\n",
    "years = np.arange(2017, 2022)  # 使用2017-2020全数据\n",
    "# years = np.arange(2017, 2021)  # 使用2017-2019数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 years 和 year_table 的 quantity 做 二次曲线拟合(同理可做三次…) 得到 annual_growth 的方程\n",
    "\n",
    "annual_growth = np.poly1d(np.polyfit(years[:-1], year_table.values.squeeze(), 2, w=np.exp((years - 2021) / 10)[:-1]))  # 使用2017-2020全数据\n",
    "# annual_growth = np.poly1d(np.polyfit(years[:-1], year_table.values.squeeze(), 2, w=np.exp((years - 2020) / 10)[:-1]))  # 使用2017-2019全数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_quantity = []\n",
    "for index, row in test.iterrows():\n",
    "    dayofweek, month, year = row.name.dayofweek, row.name.month, row.name.year  # 用 row.name 返回时间\n",
    "    itemid, provinceid = row['item_id'], row['province_id']\n",
    "    base_quantity = dayofweek_itemid_table.at[dayofweek, itemid]\n",
    "    monthmulprovince = month_table.at[month, 'quantity'] * provinceid_table.at[provinceid, 'quantity']\n",
    "    thingstoappend = np.round(base_quantity * monthmulprovince * annual_growth(year), 0)\n",
    "    if thingstoappend == thingstoappend:\n",
    "        pred_quantity.append(int(thingstoappend))\n",
    "    else:\n",
    "        pred_quantity.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['quantity'] = pred_quantity\n",
    "train = train.loc[(train.index.year >= 2017) & (train.index.year <= 2020), :]  # 只使用2017及以后的数据\n",
    "# train = train.loc[(train.index.year >= 2017) & (train.index.year <= 2019), :]  # 只使用2017-2019的数据\n",
    "df = pd.concat([train, test], sort=False)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datefeatures(df):  # 对时间的特征做更深的研究\n",
    "    df['month'] = df.datetime.dt.month\n",
    "    df['day_of_month'] = df.datetime.dt.day\n",
    "    df['day_of_year'] = df.datetime.dt.dayofyear\n",
    "    df['week_of_year'] = df.datetime.dt.weekofyear\n",
    "    df['day_of_week'] = df.datetime.dt.dayofweek + 1\n",
    "    df['year'] = df.datetime.dt.year\n",
    "    df['is_weekend'] = df.datetime.dt.weekday // 4\n",
    "    df['is_month_start'] = df.datetime.dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df.datetime.dt.is_month_end.astype(int)\n",
    "    df['quarter'] = df.datetime.dt.quarter\n",
    "    df['week_until_now'] = [\n",
    "        int(x) for x in np.floor(\n",
    "            (df.datetime - pd.to_datetime('2016-12-31')).dt.days / 7) + 1\n",
    "    ]\n",
    "    df['quarter_until_now'] = (df['year'] - 2017) * 4 + df['quarter']\n",
    "    df['week_of_month'] = df['week_of_year'].values // 4.35\n",
    "    return df\n",
    "\n",
    "\n",
    "df = datefeatures(df)\n",
    "\n",
    "weekofdaylist = [\n",
    "    'is_Mon', 'is_Tue', 'is_Wed', 'is_Thu', 'is_Fri', 'is_Sat', 'is_Sun'\n",
    "]\n",
    "for i in range(7):\n",
    "    df[weekofdaylist[i]] = np.where(df['day_of_week'] == i + 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurelist = [\n",
    "    'day_of_week', 'week_of_month', 'week_of_year', 'month', 'quarter',\n",
    "    'is_weekend', 'day_of_week', 'week_of_month'\n",
    "]\n",
    "shiftlist = [0, 0, 0, 0, 0, 0, 12, 12]  # 设置偏移量数组\n",
    "\n",
    "for feature, shift in zip(featurelist, shiftlist):\n",
    "    grouped_df = df.groupby(['province_id', 'item_id', feature])['quantity'].expanding().mean().shift(shift).bfill().reset_index()\n",
    "    grouped_df.columns = [\n",
    "        'province_id', 'item_id', feature, 'datetime',\n",
    "        feature + f'_ex_avg_quan{str(shift)}'\n",
    "    ]\n",
    "    grouped_df = grouped_df.sort_values(\n",
    "        by=['item_id', 'province_id', 'datetime'])\n",
    "    df[feature + f'_ex_avg_quan{str(shift)}'] = grouped_df[\n",
    "        feature + f'_ex_avg_quan{str(shift)}'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['item_id', 'province_id', 'datetime'], axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# 生成噪音\n",
    "def random_noise(df):\n",
    "    return np.random.normal(scale=0.01, size=(len(df), ))\n",
    "\n",
    "\n",
    "# 对不同的时间窗口生成滞后特征\n",
    "def lag_features(df, laglist):\n",
    "    df = df.copy()  # 深拷贝, 将函数里面的拷贝传递到函数外面\n",
    "    for lag in laglist:\n",
    "        df['quan_lag_' + str(lag)] = df.groupby([\"item_id\", \"province_id\"])['quantity'].transform(lambda x: x.shift(lag)) + random_noise(df)\n",
    "        # 生成随机的滞后特征, 值是通过shift函数的移位加上随机噪音\n",
    "    return df\n",
    "\n",
    "\n",
    "laglist = [91, 98, 105, 112, 119, 126, 182, 364, 546, 728]\n",
    "df = lag_features(df, laglist)\n",
    "\n",
    "\n",
    "# 移动平均, 即用当前值和前2个数值取平均数, 再加上随机噪音\n",
    "def roll_mean_features(df, windows):\n",
    "    df = df.copy()\n",
    "    for window in windows:\n",
    "        df['quan_roll_mean_' + str(window)] = df.groupby([\"item_id\", \"province_id\"])['quantity'].transform(lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "windows = [91, 182, 365, 546, 730]\n",
    "df = roll_mean_features(df, windows)\n",
    "\n",
    "\n",
    "# 指数加权平均特征\n",
    "def ewm_features(df, alphas, lags):\n",
    "    df = df.copy()\n",
    "    for alpha in alphas:\n",
    "        for lag in lags:\n",
    "            df['quan_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n",
    "            df.groupby([\"item_id\", \"province_id\"])['quantity'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n",
    "    return df\n",
    "\n",
    "\n",
    "alphas = [0.95, 0.9, 0.8, 0.7, 0.5]\n",
    "lags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n",
    "df = ewm_features(df, alphas, lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "df_dum = pd.get_dummies(df[[\n",
    "        'province_id',\n",
    "        'item_id',\n",
    "        'day_of_week',\n",
    "        'month',\n",
    "        ]],\n",
    "        columns=[\n",
    "        'province_id',\n",
    "        'item_id',\n",
    "        'day_of_week',\n",
    "        'month',\n",
    "        ],\n",
    "        dummy_na=True)\n",
    "df = pd.concat([df, df_dum], axis=1)\n",
    "\n",
    "# 转用 log\n",
    "df['quantity'] = np.log1p(df[\"quantity\"].values)\n",
    "\n",
    "# 特征工程与数据准备完成\n",
    "print(f'用时 {int(time.time()-start_time)} 秒.')\n",
    "print(f'共有 {df.shape[1]} 种特征 <=---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型检验(大约耗时3分钟)\n",
    "start_time = time.time()\n",
    "\n",
    "cols = [\n",
    "    col for col in df.columns\n",
    "    if col not in ['datetime', 'id', \"quantity\", \"year\"]\n",
    "]\n",
    "\n",
    "train = df.loc[~df.quantity.isna()]\n",
    "X_train, Y_train = train[cols], train['quantity']\n",
    "\n",
    "test = df.loc[df.id.notnull()]\n",
    "X_test = test[cols]\n",
    "\n",
    "iteration = 15000\n",
    "\n",
    "lgb_params = {\n",
    "    'nthread': -1,\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': 7,\n",
    "    'num_leaves': 28,\n",
    "    'task': 'train',\n",
    "    'objective': 'regression_l1',\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'lambda_l1': 0.06,\n",
    "    'lambda_l2': 0.05,\n",
    "    'verbose': -1,\n",
    "}\n",
    "\n",
    "# LightGBM dataset\n",
    "lgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n",
    "final_model = lgb.train(lgb_params, lgbtrain_all, num_boost_round=iteration)\n",
    "test_preds = final_model.predict(X_test, num_iteration=iteration)\n",
    "print(f'模型计算用时 {int(time.time()-start_time)} 秒.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': [*range(len(test_preds))],\n",
    "    'quantity': np.round(np.expm1(test_preds), 7)\n",
    "})  # turn back to normal scale\n",
    "submission.quantity[submission.quantity < 0] = 0\n",
    "submission['quantity'] = submission.quantity.astype(float)\n",
    "submission.to_csv('submission1.csv', index=False)\n",
    "# submission.to_csv('submission2.csv', index=False)\n",
    "# submission.to_csv('submission3.csv', index=False)\n",
    "# submission.to_csv('submission1_ex2020.csv', index=False)\n",
    "# submission.to_csv('submission2_ex2020.csv', index=False)\n",
    "# submission.to_csv('submission3_ex2020.csv', index=False)\n",
    "print(f'单日预测结果数据已导出.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 若考虑缺失值, 最终我们可以得到两组数据, 一组是包括疫情年的数据(即不考虑疫情影响, submission1.csv, submission2.csv, submission3.csv), 一组是不包括疫情年的数据(即考虑疫情影响, submission1_ex2020.csv, submission2_ex2020.csv, submission3_ex2020.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)若不考虑缺失值, 我们可以得到一组数据(submission.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于组内不同方法得出的结果采用的权重为[30%, 16%, 54%], 两组间的权重为[86%, 14%], 与不考虑缺失值的结果权重为[50%, 50%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将最终结果作为 predict_quantity 列加入到大赛要求的 submission.csv 文件中"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
